{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project with Kaggle Datasets Used by Peers\n",
    "\n",
    "dataset url : https://www.kaggle.com/datasets/lantian773030/pokemonclassification\n",
    "\n",
    "### Task : Classifying 150 Species of Pokémon Using Image Classification\n",
    "\n",
    "* The goal of this project is to train image classification tasks on 150 species of Pokémon using pretrained models such as AlexNet, VGG19, GoogleNet, and ResNet50. The objective is to improve the performance of classification tasks compared to existing machine learning models.\n",
    "* 이 프로젝트의 목표는 pretrained된 AlexNet, VGG19, GoogleNet, ResNet50을 이용하여 포켓몬 150 종류의 이미지를 분류하는 작업을 학습시키고, 기존 머신러닝 모델 대비 분류 작업의 성능을 개선하는 것입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 데이터셋 설명\n",
    "* 해당 데이터 셋은 150가지 종류의 포켓몬 라벨별 40 ~ 60장의 이미지 데이터로 이루어져있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"/content/PokemonData/\"\n",
    "classes = os.listdir(train_data_dir)\n",
    "classes = {k: v for k,v in enumerate(sorted(classes))}\n",
    "print(classes)\n",
    "\n",
    "#출력 : {0: 'Abra', 1: 'Aerodactyl', 2: 'Alakazam', ...  147: 'Wigglytuff', 148: 'Zapdos', 149: 'Zubat'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 선택한 데이터 셋은 아래와 같은 두 가지의 데이터 셋으로 나누어 학습을 진행하였습니다.\n",
    "    * train\n",
    "    * validate\n",
    "    * 데이터 분류 라벨 갯수가 많고, 각 라벨별 데이터가 많지 않으므로 train / validate 두 종류로 분류하여 학습을 진행하였습니다.\n",
    "        \n",
    "* 모델 학습 시 데이터 셋은 다음과 같이 분류하여 사용하였습니다.\n",
    "    * 전체 학습 데이터 : 6820장의 이미지\n",
    "    * train data : 학습 데이터 (5456장의 이미지 - 전체 데이터의 80%)\n",
    "    * validation data : 모델 성능 검증을 위한 데이터  (1364장의 이미지 - 전체 데이터의 20%)\n",
    "\n",
    "\n",
    "### Dataset Description\n",
    "* This dataset consists of images of 150 different Pokémon labels, with each label having 40 to 60 images.\n",
    "\n",
    "* The chosen dataset is divided into two subsets for the training process:\n",
    "    * Train\n",
    "    * Validate\n",
    "    * Due to a large number of classification labels and a relatively small number of images per label, the dataset is split into 'train' and 'validate' subsets for effective training.\n",
    "\n",
    "* During the model training, the dataset is categorized as follows:\n",
    "    * Total training data: 6820 images\n",
    "    * Train data: Used for model training, comprising 5456 images (80% of the total dataset)\n",
    "    * Validation data: Utilized for assessing model performance, consisting of 1364 images (20% of the total dataset)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 및 테스트 환경\n",
    "\n",
    "* 구글 Colab에서 T4GPU를 사용하여 모델 학습 및 테스트 진행.\n",
    "* 로컬 환경에서 GPU를 이용하여 모델 학습 및 테스트 진행.\n",
    "\n",
    "### 학습 모델 선택 시의 고려 사항\n",
    "\n",
    "1. 이전 Midterm Project에서 Batch Norminazation, Dropout, Optimization(Adam 사용) 등을 사용하며 진행한 바, 이번 Final Project에서는 이전에 사용해보지 않았고, 새롭게 학습한 데이터 전처리, 전이 학습을 사용에 집중하였습니다.\n",
    "2. 특히 Existing Model 목차에서 학습한 모델들이 이미지 분류에 효과적인 성능을 보였던 모델들이므로 이를 이용하여 데이터 분류 성능을 향상시키고자 하였습니다.\n",
    "3. 각각의 모델들은 Fine-Tuning을 이용하여 사용할 포켓몬 이미지 데이터에 맞게 조정하는 방법으로 학습을 진행하였습니다.\n",
    "\n",
    "\n",
    "### 이미지 분류를 위해 선택한 학습 모델\n",
    "\n",
    "\n",
    "1. AlexNet\n",
    "    * 2012년 ImageNet Large Scale Visual Recognition Challenge에서 우승하여 주목받은 딥 뉴럴 네트워크 아키텍처\n",
    "    * 5 개의 Convolution 레이어와 3 개의 완전 연결 레이어로 구성\n",
    "    * ReLU (Rectified Linear Unit) 활성화 함수를 사용\n",
    "    * 오버피팅을 방지하기 위해 완전 연결 레이어에서 드롭아웃을 사용\n",
    "2. VGG-19\n",
    "    * Oxford Visual Geometry Group에서 개발한 모델\n",
    "    * 9 개의 Convolution 레이어와 3 개의 완전 연결 레이어로 구성\n",
    "    * 작은 필터 크기(3x3)의 컨볼루션 레이어를 중첩하여 깊이를 확장\n",
    "3. GoogLeNet\n",
    "    * 구글넷은 Google이 개발한 인셉션(Inception)이라 불리는 모듈을 사용한 네트워크 구조\n",
    "    * 인셉션 모듈을 통해 다양한 필터 크기를 병렬로 적용하여 네트워크를 설계\n",
    "    * 1x1 컨볼루션을 사용하여 차원 감소를 수행하고, 병렬적으로 특징을 추출\n",
    "    * 네트워크 깊이로 인한 기울기 소실 문제를 완화하기 위해 auxiliary classifiers를 사용\n",
    "4. ResNet50\n",
    "    * residual network에 대안을 적용하여 더 깊은 네트워크를 구축할 수 있도록 한 모델\n",
    "    * 1x1, 3x3, 1x1 크기의 다양한 필터를 사용하여 깊이와 넓이를 확장\n",
    "    * 이로 인해 특성이 다음 레이어로 더 잘 전달되어 컴퓨터 비전에서 정확도를 매우 향상시킴\n",
    "\n",
    "### Training and Testing Environment\n",
    "\n",
    "* Model training and testing were conducted using a T4 GPU on Google Colab and locally leveraging GPU resources.\n",
    "\n",
    "### Considerations in Model Selection\n",
    "\n",
    "1. In the previous Midterm Project, we focused on techniques such as Batch Normalization, Dropout, and Optimization (using Adam). For this Final Project, the emphasis shifted towards exploring new data preprocessing techniques and utilizing transfer learning.\n",
    "2. Particularly, in the Existing Model section, we chose models that had demonstrated effective performance in image classification. The goal was to enhance data classification performance using these well-performing models.\n",
    "3. Each model underwent fine-tuning to adjust to the specific Pokémon image dataset, focusing on improving data classification performance.\n",
    "\n",
    "### Selected Training Models for Image Classification\n",
    "\n",
    "1. **AlexNet**\n",
    "    * Deep neural network architecture that gained prominence by winning the 2012 ImageNet Large Scale Visual Recognition Challenge.\n",
    "    * Comprises 5 convolution layers and 3 fully connected layers.\n",
    "    * Uses Rectified Linear Unit (ReLU) activation functions.\n",
    "    * Implements dropout in fully connected layers to prevent overfitting.\n",
    "\n",
    "2. **VGG-19**\n",
    "    * Model developed by the Oxford Visual Geometry Group.\n",
    "    * Consists of 9 convolution layers and 3 fully connected layers.\n",
    "    * Achieves depth by stacking convolution layers with a small filter size (3x3).\n",
    "\n",
    "3. **GoogLeNet**\n",
    "    * Utilizes the Inception module developed by Google, applying various filter sizes in parallel for network design.\n",
    "    * Employs 1x1 convolutions for dimension reduction, extracting features in parallel.\n",
    "    * Addresses the vanishing gradient problem associated with deep networks by using auxiliary classifiers.\n",
    "\n",
    "4. **ResNet50**\n",
    "    * Model that applies the concept of residual networks to build deeper networks.\n",
    "    * Expands depth and width using various filter sizes (1x1, 3x3, 1x1).\n",
    "    * Enhances feature propagation to the next layer, significantly improving accuracy in computer vision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 필요한 라이브러리 import - Importing Essential Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Colab 환경에 데이터 저장 (kaggle API 사용) - Installing Kaggle API and Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d samuelcortinhas/muffin-vs-chihuahua-image-classification\n",
    "!unzip  muffin-vs-chihuahua-image-classification.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 학습 관련 변수 사전 설정 - Setting up training-related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/content/train'          # 이미지를 가져올 디렉토리 위치\n",
    "CATEGORIES = ['chihuahua','muffin'] # 이미지를 구분할 카테고리\n",
    "IMG_SIZE=100                        # 데이터 전처리 시 정규화할 이미지 크기 지정\n",
    "\n",
    "# 학습시킬 이미지 목록을 저장할 리스트\n",
    "training_data=[]\n",
    "# 학습시킬 이미지의 이미지 데이터를 저장할 리스트\n",
    "X=[]\n",
    "# 학습시킬 이미지의 레이블 데이터를 저장할 리스트\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. 훈련 데이터 생성 - Create train dataset\n",
    "\n",
    "* 'chihuahua'와 'muffin' 범주에서 이미지 데이터를 읽어들이고, 크기를 조정한 후 'training_data' 리스트에 [이미지, 레이블] 형식으로 저장합니다. 'class_num' 변수는 레이블을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path=os.path.join(DATADIR, category)\n",
    "        class_num=CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array=cv2.imread(os.path.join(path,img))\n",
    "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "                training_data.append([new_array,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "     \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. 데이터 전처리 - Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenofimage = len(training_data)\n",
    "\n",
    "# 'X' 리스트에 이미지 데이터를 저장하고, \n",
    "# 'y' 리스트에 레이블 데이터를 저장합니다.\n",
    "for categories, label in training_data:\n",
    "    X.append(categories)\n",
    "    y.append(label)\n",
    "\n",
    "# 'X'와 'y' 데이터를 NumPy 배열로 변환하고, 이미지 데이터를 0-1 범위로 정규화합니다.\n",
    "X= np.array(X).reshape(lenofimage,-1)\n",
    "# 배열을 단순화 합니다. 예) [1,2,[3,4,[5,6,]]] => [1,2,3,4,5,6]\n",
    "X = X/255.0\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6. 데이터 분할 - Split train data to train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y) # 학습시킬 데이터와 학습 후 테스트할 데이터로 분류합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7. 모델 로드 후 학습 진행 - Load SVC model and Perform Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear',gamma='auto') # 커널은 선형 커널을 사용하고, 감마 값은 자동으로 계산되도록 합니다.\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8. 학습된 결과를 이용하여 모델 테스트 후 결과 출력 - Test Trained Model and Get the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = svc.predict(X_test)\n",
    "\n",
    "# 모델의 정확도 계산 - Calculate Model's Accuracy Score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy on data is\",accuracy_score(y_test,y2))\n",
    "\n",
    "# 세부적인 Test 결과 출력 - Get Detail Test Result\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Accuracy on data is\",classification_report(y_test,y2))\n",
    "\n",
    "# test 로그 출력 (0: 치와와, 1: 머핀)\n",
    "result = pd.DataFrame({'original' : y_test,'predicted' : y2})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.8.1 출력된 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Accuracy on data is 0.6942567567567568\n",
    "\n",
    "Accuracy on         data is              precision     recall   f1-score   support\n",
    "\n",
    "                                            0           0.69      0.80      0.74       640\n",
    "                                            1           0.70      0.58      0.63       544\n",
    "\n",
    "                            accuracy                                        0.69      1184\n",
    "                            macro avg                   0.70      0.69      0.69      1184\n",
    "                            weighted avg                0.70      0.69      0.69      1184\n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"./Images/5555.PNG\"  width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 필요한 라이브러리 import - Importing Essential Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Colab 환경에 데이터 저장 (kaggle API 사용) - Installing Kaggle API and Data Download : 1.2 항목과 동일한 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. 학습 관련 변수 사전 설정 - Setting up training-related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001 # 학습률\n",
    "batch_size = 100 # 한번에 얼만큼의 이미지를 학습시킬지\n",
    "epochs = 10 # 데이터를 몇 번 반복하여 학습시킬지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 데이터 전처리 - Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 이미지 데이터를 일괄적으로 300 * 300 의 크기로 정규화하고, \n",
    "# 텐서로 변환되도록 데이터 전처리 파이프라인을 정의합니다.\n",
    "trans = transforms.Compose([transforms.Resize((300, 300)),\n",
    "                           transforms.ToTensor()\n",
    "                           ])\n",
    "\n",
    "# 각 이미지 폴더에 위치한 이미지들을 정규화한 데이터셋으로 가져옵니다.\n",
    "train_dataset = torchvision.datasets.ImageFolder(root= '/content/train',\n",
    "                                                     transform = trans)\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(root= '/content/test',\n",
    "                                                     transform = trans)\n",
    "\n",
    "# train 데이터를 train / validation 데이터가 랜덤한 두 가지 값으로 나뉘도록 합니다.\n",
    "val_size = 1000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_data,val_data = random_split(train_dataset,[train_size,val_size])\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "# 각 학습용 데이터들을 사전 정의한 batch_size 단위로 처리할 수 있도록 데이터 로드를 생성합니다.\n",
    "train_loader = DataLoader(dataset=train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_data,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)\n",
    "\n",
    "print(len(train_data), len(train_loader))\n",
    "print(len(val_data), len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 CNN 모델 정의 - CNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cnn,self).__init__()\n",
    "\n",
    "        # 첫번째 레이어 정의\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # 이미지를 입력값으로 받으므로, 최초 입력 채널의 크기는 3입니다.(RGB)\n",
    "            # CNN에서 가장 기본적으로 사용되는 3*3 크기의 커널을 필터로 선택합니다.\n",
    "            nn.Conv2d(3,16,kernel_size=3, padding=0,stride=2),\n",
    "            # BatchNorm2d를 이용하여 학습 시 사용되는 batch를 정규화합니다.\n",
    "            # 정규화 과정을 통해 학습을 더 안정적으로 진행할 수 있습니다.\n",
    "            nn.BatchNorm2d(16),\n",
    "            # 활성화 함수로는 Relu를 사용합니다.\n",
    "            nn.ReLU(),\n",
    "            # 2x2 Max 풀링을 수행하여 가장 큰 특징을 추출합니다.\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # 두번째 레이어 정의\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16,32, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "        \n",
    "        # 세번째 레이어 정의\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32,64, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # MLP 레이어 정의\n",
    "        # 출력된 최종 레이어를 하나로 쭉 늘어뜨린 뒤, 10개의 출력층에 맵핑합니다.\n",
    "        self.fc1 = nn.Linear(1296,10)\n",
    "        # Dropout 레이어는 학습 중 일부 뉴런을 무작위로 비활성화하여 과적합을 방지합니다.\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # 위에서 10개로 줄어든 출력층을 최종 muffin/chihuahua 2개의 출력층으로 분류합니다.\n",
    "        self.fc2 = nn.Linear(10,2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # 상단에서 정의해준 각각의 레이어를 실행시키는 순방향전파 기능을 정의합니다.\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 모델 학습 준비 - Prepare for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 : 상단에서 정의한 모델 활용\n",
    "model = Cnn().cuda()\n",
    "# 모델 학습 모드 설정\n",
    "model.train()\n",
    "# 최적화 함수 설정 : Adam(Adaptive Moment Estimation)\n",
    "# Adam? : Pytorch에 구현되어있는 경사하강법의 변형 중 하나\n",
    "optimizer = optim.Adam(params = model.parameters(),lr=0.001)\n",
    "# 손실 함수는 CrossEntropy함수를 사용합니다.\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7. 모델 학습 수행 - Perform Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    # 학습 데이터 로더 train_loader에서 batch 데이터를 가져옵니다.\n",
    "    for i, (batch_images, batch_labels) in enumerate(train_loader):\n",
    "        output = model(batch_images)\n",
    "        loss = criterion(output, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()   # 최적화 함수 가중치 초기화\n",
    "        loss.backward()         # 역전파 수행하여 가중치 계산하고, 모델의 파라미터를 업데이트\n",
    "        optimizer.step()        # 최적화 함수의 파라미터도 업데이트\n",
    "\n",
    "        acc = ((output.argmax(dim=1) == batch_labels).float().mean())   # 예측값의 정확도를 계산합니다.\n",
    "        epoch_accuracy += acc/len(train_loader)                         # 지금 수행중인 학습의 정확도와 손실을 누적합니다.\n",
    "        epoch_loss += loss/len(train_loader)\n",
    "\n",
    "    print('Epoch : {}, train accuracy : {}, train loss : {}'.format(epoch+1, epoch_accuracy,epoch_loss))\n",
    "\n",
    "\n",
    "    with torch.no_grad():       # validation data로 학습에 대한 검증을 수행합니다.\n",
    "        epoch_val_accuracy=0\n",
    "        epoch_val_loss =0\n",
    "        for data, label in val_loader:\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output,label)\n",
    "\n",
    "\n",
    "            acc = ((val_output.argmax(dim=1) == label).float().mean())\n",
    "            epoch_val_accuracy += acc/ len(val_loader)\n",
    "            epoch_val_loss += val_loss/ len(val_loader)\n",
    "\n",
    "        print('Epoch : {}, val_accuracy : {}, val_loss : {}'.format(epoch+1, epoch_val_accuracy,epoch_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7.1 출력 확인\n",
    "\n",
    "```\n",
    "Epoch : 1, train accuracy : 0.8281340003013611, train loss : 0.4042232036590576\n",
    "Epoch : 1, val_accuracy : 0.8700001239776611, val_loss : 0.31266215443611145\n",
    "Epoch : 2, train accuracy : 0.8902550935745239, train loss : 0.27626872062683105\n",
    "Epoch : 2, val_accuracy : 0.8899999856948853, val_loss : 0.2872754633426666\n",
    "Epoch : 3, train accuracy : 0.9102311134338379, train loss : 0.2230011373758316\n",
    "Epoch : 3, val_accuracy : 0.9120000004768372, val_loss : 0.2264741063117981\n",
    "Epoch : 4, train accuracy : 0.9289393424987793, train loss : 0.18602629005908966\n",
    "Epoch : 4, val_accuracy : 0.9070000648498535, val_loss : 0.2519312798976898\n",
    "Epoch : 5, train accuracy : 0.9378626942634583, train loss : 0.1698230803012848\n",
    "Epoch : 5, val_accuracy : 0.9110000133514404, val_loss : 0.23171471059322357\n",
    "Epoch : 6, train accuracy : 0.9478866457939148, train loss : 0.14132191240787506\n",
    "Epoch : 6, val_accuracy : 0.9110000133514404, val_loss : 0.2093384712934494\n",
    "Epoch : 7, train accuracy : 0.9589393138885498, train loss : 0.11629442870616913\n",
    "Epoch : 7, val_accuracy : 0.9200000762939453, val_loss : 0.21673990786075592\n",
    "Epoch : 8, train accuracy : 0.9639395475387573, train loss : 0.10555274784564972\n",
    "Epoch : 8, val_accuracy : 0.9149999618530273, val_loss : 0.21225492656230927\n",
    "Epoch : 9, train accuracy : 0.9802554845809937, train loss : 0.07289625704288483\n",
    "Epoch : 9, val_accuracy : 0.9070000052452087, val_loss : 0.2515607476234436\n",
    "Epoch : 10, train accuracy : 0.973684549331665, train loss : 0.07742644846439362\n",
    "Epoch : 10, val_accuracy : 0.9120000004768372, val_loss : 0.24639680981636047\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8. 학습 결과 테스트 - Test Model\n",
    "\n",
    "* 학습한 모델을 evaluation 모드로 설정한 후, 테스트용 사진 목록 중 랜덤한 사진을 뽑아 실제 라벨값과 모델이 예측한 라벨값을 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 테스트 데이터셋에서 무작위 이미지 선택\n",
    "random_image_idx = random.randint(0, len(test_data) - 1)\n",
    "image, label = test_data[random_image_idx]\n",
    "\n",
    "# 순방향전파를 실행하여 예측 획득\n",
    "with torch.no_grad():\n",
    "    image = image.unsqueeze(0).cuda()\n",
    "    prediction = model(image)\n",
    "\n",
    "predicted_class = prediction.argmax().item()\n",
    "\n",
    "# 클래스 레이블 목록 정의 (데이터셋에 따라 사용자 정의 가능)\n",
    "class_labels =['chihuahua', 'muffin']\n",
    "\n",
    "# 이미지와 예측을 표시\n",
    "plt.imshow(image.squeeze(0).cpu().permute(1, 2, 0))\n",
    "plt.title(f'Actual: {class_labels[label]}, Predicted: {class_labels[predicted_class]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.8.1. 출력 확인\n",
    "\n",
    "<img src=\"./Images/1111.PNG\"  width=\"50%\" height=\"50%\"></img>\n",
    "<img src=\"./Images/2222.PNG\"  width=\"50%\" height=\"50%\"></img>\n",
    "<img src=\"./Images/3333.PNG\"  width=\"50%\" height=\"50%\"></img>\n",
    "<img src=\"./Images/4444.PNG\"  width=\"50%\" height=\"50%\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 총평\n",
    "\n",
    "* 목적에 따른 알고리즘의 효율성\n",
    "    * 머핀과 치와와 같은 특징적인 이미지를 학습시키고, 학습 기준에 따라 대상을 구분할 수 있도록 하고자 하였을 때는 CNN 알고리즘이 SVC보다 효과적이었습니다.\n",
    "\n",
    "* 훈련에 사용될 데이터 수집 및 전처리 과정의 중요성\n",
    "    * 선택한 오픈 데이터셋이 구글 이미지 검색 결과를 스크래핑한 것이어서 학습/테스트 데이터 자체에 '치와와', 혹은 '머핀'이 아닌 이미지가 섞여있어 더 효과적인 훈련이 진행되지 못한 것으로 보입니다.\n",
    "    \n",
    "* 최적화 알고리즘 선택의 중요성\n",
    "    * CNN 알고리즘의 경우, Adam 대신 SGD 최적화 함수를 사용할 경우 정확도가 약 70%까지 떨어졌습니다.\n",
    "\n",
    "## Overall Summary\n",
    "\n",
    "* Efficiency of the Algorithm Based on the Objective\n",
    "   * When the goal was to train and differentiate distinctive images like muffins and Chihuahuas, the CNN algorithm proved to be more effective than SVC.\n",
    "\n",
    "* Importance of Data Collection and Preprocessing in Training\n",
    "   * The selected open dataset appeared to be scraped from Google image search results and contained images that were not actually \"Chihuahua\" or \"muffin,\" which hindered effective training.\n",
    "\n",
    "* Significance of Algorithm Selection and Optimization\n",
    "   * In the case of the CNN algorithm, using the SGD optimization function instead of Adam resulted in a decrease in accuracy to around 70%."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
